<html>

<head>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
</head>

<body>
<div class="nav">
  <a class="active" href="/">Home</a>
  <a href="/value_iteration">Value Iteration</a>
  <a href="/policy_iteration">Policy Iteration</a>
</div>

<div>
<h2>Dynamic Programming for Solving MDP's</h2>

<p>This website serves as an introduction to Markov decision processes and includes interactive demonstrations for a few algorithms used to solve them. Code for setting up and solving MDP's can be found here: <a href="https://github.com/bmontambault/mdp">https://github.com/bmontambault/mdp</a></p>

<h3>What is a Markov decision process?</h3>

<p>Suppose we have a graph where traversing each edge gives us some reward (positive or negative), and suppose our goal is to traverse this graph in such a way that the sum of these rewards is maximized over some (possibly infinite) number of moves. Given we are at the vertex <img src="http://latex.codecogs.com/gif.latex?s" border="0"/>, how do we choose which edge, <img src="http://latex.codecogs.com/gif.latex?(s,s')" border="0"/>, to traverse next? If we only cared about reward on our next action, the obvious choice is to just choose the edge with the highest reward. However, since we care about future rewards as well, we have to take into account the rewards we'll have access to once we're at <img src="http://latex.codecogs.com/gif.latex?s'" border="0"/>. Markov decision processes address this problem by assigning each vertex (or "state") a value, given by the recursive definition

<div lang="latex">
V^{\pi}(s) = \sum_{s'}P(s, \pi(s), s')(R(s, \pi(s), s') + \gamma V^{\pi}(s'))
</div>

Where <img src="http://latex.codecogs.com/gif.latex?\pi(s)" border="0"/> is our "policy" for state <img src="http://latex.codecogs.com/gif.latex?s" border="0"/>, and determines the action we will take any time that we end up at this state,
<img src="http://latex.codecogs.com/gif.latex?P(s, \pi(s), s'))" border="0"/> is the probability that we end up at state <img src="http://latex.codecogs.com/gif.latex?s'" border="0"/> after taking action <img src="http://latex.codecogs.com/gif.latex?\pi(s)" border="0"/>, <img src="http://latex.codecogs.com/gif.latex?R(s, \pi(s), s'))" border="0"/> is the reward we get from transitioning from state <img src="http://latex.codecogs.com/gif.latex?s" border="0"/> to state <img src="http://latex.codecogs.com/gif.latex?s'" border="0"/> taking action <img src="http://latex.codecogs.com/gif.latex?\pi(s)" border="0"/>, and <img src="http://latex.codecogs.com/gif.latex?\gamma" border="0"/> is a discount factor between 0 and 1 that encodes how much we prefer immediate rewards over future rewards.
</p>

<p>
The optimal policy for each state is one that transitions to the adjacent state with the highest value, or

<div lang="latex">
\pi(s) = \text{argmax}_{a} \left\{\sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V^(s')) \right\}
</div>
</p>

<p>
Solving an MDP therefore requires finding the value <img src="http://latex.codecogs.com/gif.latex?V(s)" border="0"/> and policy <img src="http://latex.codecogs.com/gif.latex?\pi(s)" border="0"/> for each state.
</p>

<h3>Algorithms</h3>

<p>Dynamic programming is a method for solving problems that have the following characteristics:</p>

<p>A) Optimal substructure: The optimal solution can be found by finding the optimal solutions to smaller subproblems.</p>

<p>The recursive definition of the value function gives a state's value as the sum of the value and reward at all other states, times the probability of reaching that state from the current state. Therefore, if we've solved for the value function of all states adjacent to <img src="http://latex.codecogs.com/gif.latex?s" border="0"/>, solving for <img src="http://latex.codecogs.com/gif.latex?V(s)" border="0"/> only requires combining these with our known reward and transition functions. </p>

<p>B) Overlapping subproblems: Solutions to subproblems are saved and reused.</p>

<p>
Once we have calculated the value function at a state, that value can be reused to solve for the value function at all adjacent states.
</p>

<p>
In Markov decision processes, solving for the value of any state requires knowing the values of adjacent states. Once the value of any one state has been determined, it can be used to help solve for the value of any adjacent state. MDP's therefore have both optimal substructure and overlapping subproblems. The two most popular algorithms for solving MDP's, <a href="/value_iteration"> value iteration</a> and <a href="/policy_iteration">policy iteration</a>, are both applications of dynamic programming.
</p>

<h3>Grid World</h3>
<p>
Both algorithms can be demonstrated using a simple 2d environment. This environment makes a few simplifications to the standard MDP. First, we make it so the reward gained from visiting a state depends only on the end state, rather than also depending on the starting state and the action taken. Second, we make the transition function deterministic, so every action has a state such that taking that action will always results in transitioning to that state.
</p>

<p>
From each state we can only move left, right, up, down, or stay. Some states (grey) cannot be traversed at all. The discount factor can be modified by entering a new number between 0 and 1 and clicking update discount The reward for any state can be modified by clicking the associated cell, entering a new reward, and clicking update reward. Any blocked state can be made un-blocked and any un-blocked state can be blocked by clicking the associated cell and then clicking toggle blocked. To see how values and policies are updated, go to either the <a href="/value_iteration">value iteration</a> or <a href="/policy_iteration">policy iteration</a> pages.

</p>


<div>
	<input type="number" id='discount' step="0.1" value=".8" min="0" max=".9" style="width: 4em"></input>
    <button type="button" id="update_discount" onclick="update_reward(selected_row, selected_col)">update discount</button>
	<input type="number" id='reward' value="0" disabled style="width: 4em"></input>
    <button type="button" id="reward_button" onclick="update_reward(selected_row, selected_col)" disabled>update reward</button>
	<button type="button" id="blocked_button" onclick="toggle_blocked(selected_row, selected_col)"disabled>toggle blocked</button>
	<div id='table'>
		{{table|safe}}
	</div>
</div>

</div>

</body>

<script type="text/javascript">

	var size = {{size}}
	var state_rewards_list = {{state_rewards_list}}
    var blocked_states_list = {{blocked_states_list}}
    var discount = {{discount}}
    var values = {{values}}
    var policy = {{policy}}

	var original_color = null
	var selected_row = null
	var selected_col = null

	var started = false
	

	$("#table").on('click', 'td', function() {

		document.getElementById("reward").disabled = false;
		document.getElementById("reward_button").disabled = false;
		document.getElementById("blocked_button").disabled = false;

	    var id = $(this).attr("id");
	    selected_row = parseInt(id[0])
	    selected_col = parseInt(id[1])

	    //save original class
	    var cell_class = $(this).attr("class");

	    //deselect other cells
	    var selected = document.getElementsByClassName('selected')
	    if (selected.length > 0){
	    	cell = selected[0]
	    	cell.classList.remove("selected")
	    	cell.style.background = original_color
	    }

	    //if it wasn't selected originally, select cell
	    if (typeof cell_class !== 'selected'){
	    	$(this).addClass('selected')
	    	original_color = $(this).css('background-color')
	    	$( this ).css( 'background-color', 'yellow' );
	    }

	});
	


	function update_table(){

		data = {"size": size,
	    		  "state_rewards_list": state_rewards_list,
	    		  "blocked_states_list": blocked_states_list,
	    		  "discount": discount,
	    		  "values": values,
	    		  "policy": policy,
	    		  "started": started}

		$.ajax({
	        method:"POST",
	        contentType: 'application/json',
	        url: '/update',
	        data:JSON.stringify(data),
	    	dataType: 'json',
          	contentType: 'application/json; charset=utf-8',
	        success:function(resp, data){
	            var response = resp;
	            var table = response['table']
	            document.getElementById('table').innerHTML = table
	            values = response['values']
	            policy = response['policy']
	        },
	    });
	}


	function update_discount(selected_row, selected_col){

		var discount = parseInt(document.getElementById('discount').value)
		state_rewards_list.push([[selected_row, selected_col], discount])
		update_table('/update');
	}


	function update_reward(selected_row, selected_col){

		var reward = parseInt(document.getElementById('reward').value)
		state_rewards_list.push([[selected_row, selected_col], reward])
		update_table();
	}


	function is_blocked(cell){

		var idx = -1
		for (var i=0; i<blocked_states_list.length; i++){
			if (cell[0] == blocked_states_list[i][0] & cell[1] == blocked_states_list[i][1]){
				idx = i
			}
		}
		return idx
	}

	function toggle_blocked(selected_row, selected_col){

		var cell = [selected_row, selected_col]
		idx = is_blocked(cell)

		if (idx == -1){
			blocked_states_list.push(cell)
		} else {
			blocked_states_list.splice(idx, 1)
		}
		update_table();
	}


</script>


<style type="text/css">
	
	td {
	    border: 1px solid;
	    width: 50px;
	    height: 50px;
	}

</style>

</html>