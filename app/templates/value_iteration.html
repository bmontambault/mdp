<html>

<head>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
</head>

<div class="nav">
  <a href="/">Home</a>
  <a class="active" href="/value_iteration">Value Iteration</a>
  <a href="/policy_iteration">Policy Iteration</a>
</div>

<div>
<h2>Value Iteration</h2>

<p>Value iteration involves two broad steps: First, we iteratively update the value of each state by solving</p>
<div lang="latex">
V_{i+1}(s) = \text{max}_{a} \sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V_{i}(s'))
</div>

<p> where <img src="http://latex.codecogs.com/gif.latex?V_{i}(s')" border="0"/> was the value for <img src="http://latex.codecogs.com/gif.latex?s'" border="0"/> computed on the previous iteration. Once this has converged for all states, meaning that <img src="http://latex.codecogs.com/gif.latex?|V_{i+1}(s) - V_{i}(s)|" border="0"/> is less than some small number, we find the optimal policy </p>

<div lang="latex">
\pi(s) = \text{argmax}_{a} \left\{\sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V^(s')) \right\}
</div>


</div>


<div>
	<p>
	In order to choose the best policy for each state (left, right, up, down, or stay) we need to take into account both the short term and long term rewards we'll can accumulate after we've reached that state. The short term reward is given to us in the reward function. The long term reward, or value function, is computed iteratively by evaluating the short and long-term reward for each state given that we take the best action on each state.
	</p>

	<p>
	As an example we look at an environment where we recieve a reward of 1 in the states (0,0) and (3,3), and no reward in any other states. The states (2,2), (2,3), and (2,4) are blocked off and cannot be traversed. We initialize the value function to be 0 for every state.
	</p>
	<div id='table1'>
		{{table1|safe}}
	</div>
	<p>
	We initialize our value function <img src="http://latex.codecogs.com/gif.latex?V_{0}(s)=0" border="0"/> for all states, and compute <img src="http://latex.codecogs.com/gif.latex?V_{1}(s)" border="0"/> as follows:

	<div lang="latex">
	V_{1}(s) = \text{max}_{a} \sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V_{0}(s'))
	</div>

	This will come out to be the reward of each state plus the reward of the best state we can get to in one move. For (0,0) and (3,3) the best we can do in one move is to stay put and collect the same reward, so the value function at these states are twice their respective rewards. For states that are adjacent to a (0,0) and (3,3), the single move is to go to one of these states and collect the reward, so the value function at these states matches the short term reward at either (0,0) or (3,3). For the remainder of the states there are no single moves that give us any reward, so the value function at these states remains 0.
	<p>
	<div id='table2'>
		{{table2|safe}}
	</div>

	<p>
	For the next iteration we compute
	<div lang="latex">
	V_{2}(s) = \text{max}_{a} \sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V_{1}(s'))
	</div>
	Here the discount factor <img src="http://latex.codecogs.com/gif.latex?\gamma" border="0"/> comes into play. If the discount factor is close to 1, <img src="http://latex.codecogs.com/gif.latex?V_{2}(s)" border="0"/> will depend heavily on <img src="http://latex.codecogs.com/gif.latex?V_{1}(s)" border="0"/>. If the discount factor is closer to 1, <img src="http://latex.codecogs.com/gif.latex?V_{2}(s)" border="0"/> will depend mostly on the immediate reward at <img src="http://latex.codecogs.com/gif.latex?s" border="0"/>. In this example we set <img src="http://latex.codecogs.com/gif.latex?\gamma=0.9" border="0"/>. For (0,0) and (3,3), our best 2 moves are still to stay put. The value function is therefore given by the reward gained from the first "stay" move plust the reward given from the second "stay" move times the discount factor. This is also true for the states adjacent to (0,0) and (3,3). For states that are two moves from (0,0) or (3,3), the value is given by the reward of the closest state adjacent to (0,0)/(3,3) which is 0 plus  the reward of (0,0)/(3,3) times the discount factor.
	</p>
	<div id='table3'>
		{{table3|safe}}
	</div>

	<p>
	For the next iteration we compute
	<div lang="latex">
	V_{3}(s) = \text{max}_{a} \sum_{s'}P(s, a, s')(R(s, a, s') + \gamma V_{2}(s'))
	</div>
	The value function at (0,0) and (3,3) are the rewards we gain from staying at these states for 3 moves weighted by the discount factor (e.g. 2 + 2(0.9) + 2(0.9^2) = 5.42). As before, all other values are given by the maximum reward we can gain in 3 moves weighted by the discount factor. For all states outside of three moves from either (0,0) or (3,3) their value remains 0.
	</p>

	<div id='table4'>
		{{table4|safe}}
	</div>

	<p>
	After some number of moves <img src="http://latex.codecogs.com/gif.latex?i" border="0"/>, evaluating <img src="http://latex.codecogs.com/gif.latex?i+1" border="0"/> moves doesn't significantly change the value function because of the accumulation of the discount factor. What we are left with is the long term reward we can expect from any state weighted by the discount factor over an infinite number of moves.
	</p>
	<div id='table5'>
		{{table5|safe}}
	</div>

	<p>
	Combining the value function for each state with the reward function (left) allows us to choose the policy at each state that will give us the highest combined short term and long term reward (right).
	</p>
	<div style="width: 100%; overflow: hidden;">
	    <div style="width: 600px; float: left;"> {{value_table6|safe}} </div>
	    <div style="margin-left: 620px;"> {{policy_table6|safe}} </div>
	</div>

</div>


<div>

	<p>
	In the environment below, click value iteration to perform one step of value iteration. You will find that at some point the values represented in each cell will stop changing, indicating that the value function has converged. At any point, click show policy to reveal the optimal policy at each state.
	</p>

	<input type="number" id='discount' step="0.1" value=".8" min="0" max=".99" style="width: 4em"></input>
    <button type="button" id="update_discount" onclick="update_reward(selected_row, selected_col)">update discount</button>
	<input type="number" id='reward' value="0" disabled></input>
    <button type="button" id="reward_button" onclick="update_reward(selected_row, selected_col)" disabled>update reward</button>
	<button type="button" id="blocked_button" onclick="toggle_blocked(selected_row, selected_col)"disabled>toggle blocked</button>
	<div id='table'>
		{{table|safe}}
	</div>
	<button type="button" onclick="step()">value iteration</button>
	<button type="button" onclick="show_policy()">show policy</button>
	<button type="button" onclick="reset()">reset</button>
</div>

<script type="text/javascript">

	var size = {{size}}
	var state_rewards_list = {{state_rewards_list}}
    var blocked_states_list = {{blocked_states_list}}
    var discount = {{discount}}
    var values = {{values}}
    var policy = {{policy}}

    var original_color = null
	var selected_row = null
	var selected_col = null

	var started = false
	

	$("#table").on('click', 'td', function() {

		document.getElementById("reward").disabled = false;
		document.getElementById("reward_button").disabled = false;
		document.getElementById("blocked_button").disabled = false;

	    var index = $( "td" ).index( this );
	    selected_row = Math.floor( ( index ) / size) + 1;
	    selected_col = ( index % size ) + 1;

	    //save original class
	    var cell_class = $(this).attr("class");

	    //deselect other cells
	    var selected = document.getElementsByClassName('selected')
	    if (selected.length > 0){
	    	cell = selected[0]
	    	cell.classList.remove("selected")
	    	cell.style.background = original_color
	    }

	    //if it wasn't selected originally, select cell
	    if (typeof cell_class !== 'selected'){
	    	$(this).addClass('selected')
	    	original_color = $(this).css('background-color')
	    	$( this ).css( 'background-color', 'yellow' );
	    }

	});


	function update_discount(selected_row, selected_col){

		var discount = parseInt(document.getElementById('discount').value)
		state_rewards_list.push([[selected_row-1, selected_col-1], discount])
		update_table('/update');
	}


	function update_reward(selected_row, selected_col){

		var reward = parseInt(document.getElementById('reward').value)
		state_rewards_list.push([[selected_row-1, selected_col-1], reward])
		update_table('/update');
	}


	function is_blocked(cell){

		var idx = -1
		for (var i=0; i<blocked_states_list.length; i++){
			if (cell[0] == blocked_states_list[i][0] & cell[1] == blocked_states_list[i][1]){
				idx = i
			}
		}
		return idx
	}

	function toggle_blocked(selected_row, selected_col){

		var cell = [selected_row-1, selected_col-1]
		idx = is_blocked(cell)

		if (idx == -1){
			blocked_states_list.push(cell)
		} else {
			blocked_states_list.splice(idx, 1)
		}
		update_table('/update');
	}

	function update_table(url){

		data = {"size": size,
	    		  "state_rewards_list": state_rewards_list,
	    		  "blocked_states_list": blocked_states_list,
	    		  "discount": discount,
	    		  "values": values,
	    		  "policy": policy,
	    		  "started": started}

		$.ajax({
	        method:"POST",
	        contentType: 'application/json',
	        url: url,
	        data:JSON.stringify(data),
	    	dataType: 'json',
          	contentType: 'application/json; charset=utf-8',
	        success:function(resp, data){
	            var response = resp;
	            var table = response['table']
	            document.getElementById("table").innerHTML = table
	            values = response['values']
	            policy = response['policy']
	        },
	    });
	}


	function step(){

		update_table("/value_iteration_step")
		started = true
	}


	function show_policy(){

		update_table("/value_iteration_show_policy")
	}


	function reset(){

		started = false
		update_table("/update")
	}


</script>


<style type="text/css">
	
	td {
	    border: 1px solid;
	    width: 50px;
	    height: 50px;
	}

</style>

</html>